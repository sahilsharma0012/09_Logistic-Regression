{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ⭐ 1️⃣ Why Linear Regression Fails for Classification\n",
    "\n",
    "Suppose we want to predict:\n",
    "\n",
    "| Hours | Pass(1) / Fail(0) |\n",
    "| ----- | ----------------- |\n",
    "| 1     | 0                 |\n",
    "| 2     | 0                 |\n",
    "| 3     | 0                 |\n",
    "| 4     | 1                 |\n",
    "| 5     | 1                 |\n",
    "\n",
    "Linear regression equation would be:\n",
    "\n",
    "[\n",
    "y = mx + c\n",
    "]\n",
    "\n",
    "But the problem:\n",
    "\n",
    "❌ It can give values like −1, 1.3, 2.1, 10 etc.\n",
    "❌ These are invalid probabilities.\n",
    "❌ Threshold becomes unreliable.\n",
    "\n",
    "So we need something that:\n",
    "\n",
    "✔ Converts all outputs between **0 to 1**\n",
    "✔ Can be interpreted as **probability**\n",
    "\n",
    "That is where **sigmoid** comes.\n",
    "\n",
    "---\n",
    "\n",
    "# ⭐ 2️⃣ Why Logistic Regression Uses Sigmoid\n",
    "\n",
    "Sigmoid squeezes any number into 0→1 range:\n",
    "\n",
    "[\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "]\n",
    "\n",
    "### What is \"z\"?\n",
    "\n",
    "[\n",
    "z = w_1x_1 + w_2x_2 + ... + b\n",
    "]\n",
    "\n",
    "This is the **same linear equation** as linear regression.\n",
    "\n",
    "But instead of giving output directly, we pass it to sigmoid:\n",
    "\n",
    "[\n",
    "p = \\sigma(z)\n",
    "]\n",
    "\n",
    "And now p = probability → 0 ≤ p ≤ 1\n",
    "\n",
    "---\n",
    "\n",
    "# ⭐ 3️⃣ Logistic Regression Final Formula\n",
    "\n",
    "[\n",
    "p = \\frac{1}{1 + e^{-(w_1x_1 + w_2x_2 + b)}}\n",
    "]\n",
    "\n",
    "This is logistic regression.\n",
    "\n",
    "So logistic regression =\n",
    "**Linear regression + Sigmoid + Probability interpretation**\n",
    "\n",
    "---\n",
    "\n",
    "# ⭐ 4️⃣ Probability → Class Conversion\n",
    "\n",
    "If p ≥ 0.5 → Class 1\n",
    "If p < 0.5 → Class 0\n",
    "\n",
    "Why 0.5?\n",
    "Because sigmoid(0) = 0.5\n",
    "\n",
    "So the **decision boundary** is:\n",
    "\n",
    "[\n",
    "w_1x_1 + w_2x_2 + b = 0\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "# ⭐ 5️⃣ Why Decision Boundary Is a Straight Line?\n",
    "\n",
    "Because boundary is formed when:\n",
    "\n",
    "[\n",
    "p = 0.5 \\Rightarrow z = 0\n",
    "]\n",
    "\n",
    "So:\n",
    "\n",
    "[\n",
    "w_1x_1 + w_2x_2 + b = 0\n",
    "]\n",
    "\n",
    "This is:\n",
    "\n",
    "* Linear equation in 2D → straight line\n",
    "* Linear equation in 3D → plane\n",
    "* Linear in nD → hyperplane\n",
    "\n",
    "This is why logistic regression is a **linear classifier**.\n",
    "\n",
    "---\n",
    "\n",
    "# ⭐ 6️⃣ Mathematical Intuition of Training (How model learns)\n",
    "\n",
    "Logistic regression does NOT use MSE (mean squared error).\n",
    "Why?\n",
    "\n",
    "Because sigmoid + MSE makes optimization unstable.\n",
    "\n",
    "So logistic regression uses **Log Loss / Cross Entropy**:\n",
    "\n",
    "[\n",
    "Loss = -[y\\log(p) + (1-y)\\log(1-p)]\n",
    "]\n",
    "\n",
    "### Case 1: y = 1\n",
    "\n",
    "Loss = -log(p)\n",
    "Higher probability → lower loss\n",
    "Lower probability → very high loss\n",
    "\n",
    "### Case 2: y = 0\n",
    "\n",
    "Loss = -log(1-p)\n",
    "\n",
    "### Meaning\n",
    "\n",
    "* If model predicts correct class → low loss\n",
    "* If prediction is confident but wrong → huge loss\n",
    "  (This forces model to correct itself)\n",
    "\n",
    "---\n",
    "\n",
    "# ⭐ 7️⃣ Gradient Descent (How weights change)\n",
    "\n",
    "Weights update rule:\n",
    "\n",
    "[\n",
    "w := w - \\alpha \\cdot \\frac{\\partial Loss}{\\partial w}\n",
    "]\n",
    "\n",
    "For logistic regression:\n",
    "\n",
    "[\n",
    "\\frac{\\partial Loss}{\\partial w} = (p - y)x\n",
    "]\n",
    "\n",
    "So:\n",
    "✔ If model predicts higher than true value → weight decreases\n",
    "✔ If model predicts lower than true value → weight increases\n",
    "\n",
    "This continues until loss becomes minimum.\n",
    "\n",
    "---\n",
    "\n",
    "# ⭐ 8️⃣ Full Mathematical Flow (Simple Summary)\n",
    "\n",
    "### Step 1 → Linear combination\n",
    "\n",
    "[\n",
    "z = w_1x_1 + w_2x_2 + ... + b\n",
    "]\n",
    "\n",
    "### Step 2 → Convert to probability\n",
    "\n",
    "[\n",
    "p = \\sigma(z)\n",
    "]\n",
    "\n",
    "### Step 3 → Calculate log-loss\n",
    "\n",
    "[\n",
    "Loss = -[y\\log(p) + (1-y)\\log(1-p)]\n",
    "]\n",
    "\n",
    "### Step 4 → Adjust weights using gradient descent\n",
    "\n",
    "[\n",
    "w := w - \\alpha(p - y)x\n",
    "]\n",
    "\n",
    "### Step 5 → Repeat until loss is minimum\n",
    "\n",
    "---\n",
    "\n",
    "# ⭐ 9️⃣ Tiny Numerical Example (Very Easy)\n",
    "\n",
    "Suppose:\n",
    "\n",
    "x = 2\n",
    "y_true = 1\n",
    "w = 0.5\n",
    "b = 0\n",
    "\n",
    "### Step 1: Linear part\n",
    "\n",
    "[\n",
    "z = 0.5 \\times 2 = 1\n",
    "]\n",
    "\n",
    "### Step 2: Sigmoid\n",
    "\n",
    "[\n",
    "p = \\frac{1}{1+e^{-1}} = 0.73\n",
    "]\n",
    "\n",
    "### Step 3: Log loss\n",
    "\n",
    "[\n",
    "Loss = -\\log(0.73) = 0.315\n",
    "]\n",
    "\n",
    "### Step 4: Gradient\n",
    "\n",
    "[\n",
    "gradient = (p - y)x = (0.73 - 1)\\times 2 = -0.54\n",
    "]\n",
    "\n",
    "### Step 5: Update weight\n",
    "\n",
    "[\n",
    "w_{new} = w - 0.1(-0.54) = 0.554\n",
    "]\n",
    "\n",
    "✔ Weight increased because prediction was lower than true value\n",
    "\n",
    "This is how logistic regression learns internally.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
